{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo livro - Approaching (almost) any Machine Learning Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando vamos desenvolver algoritmos de machine learning, podemos utilizar diversas métricas de avaliação. Algumas vezes, criamos métricas novas para atender uma necessidade de compreensão para o negócio. Nesse notebook, vamos avaliar as métricas mais comums que podem ser usadas nos mais variados tipos de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante notar que trataremos apensa de métricas de avaliaçao para algotirmos supervisionados devido a grande abundancia desse tipo de problema na indústria e o fato das métricas de avaliação de problemas não supervisionados são um pouco subjetivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As métricas mais usadas para problemas de Classificação são:\n",
    "* Accuracy \n",
    "* Precision (P)\n",
    "* Recall (R)\n",
    "* F1 score (F1)\n",
    "* Area under the ROC (Roceiver Operating Characteristic) curve ou AUC \n",
    "* Log Loss\n",
    "* Precision at k (P@k)\n",
    "* Average Precision at k (AP@k)\n",
    "* Mean Average Precision at k (MAP@k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As métricas mais usadas para problemas de Regressão são:\n",
    "* Mean Absolute Error (MAE) \n",
    "* Mean Squared Error (MSE)\n",
    "* Root Mean Squared Error (RMSE)\n",
    "* Root Mean Squared Logarithmic error (RMSLE)\n",
    "* Mean Percentage Error (MPE) \n",
    "* Mean Absolute Percentage Error (MAPE)\n",
    "* R²\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saber trabalhar com as métricas mensionadas acima não a única coisa que nós devemos saber. Além disso, devemos saber quando usar quando usar cada uma delas, e isso depende do tipo de dados que temos e do target em questão. \n",
    "\n",
    "*Nota: O autor mensiona que se \"preocupa\" mais com os targets e menos com os tipos de data para saber qual métrica escolher*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender um pouco sobre as métricas, vamos começar com um problema simples de classificação. Vamos supoer que temos um problema de classificação binária ( apenas dois targets) e tal problema consiste em classificar imagens de raio-X toráxica. Temos imagens de raio-x sem problema algum e outras com pneumatorax. Nossa tarefa é montar um algoritmo de classificação que dada uma imagem de raio-x, consiga detectar a presença de pneumatorax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'imagens/pneumotorax.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos assumir que temos dados balanceados, ou seja, o número de casos com pneumatorax é igual ao número de casos sem o problema. Se tivermos 100 casos positivos, então teremos 100 casos negativos também.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeira coisa a se fazer é separar os dados em dois sets iguais de 100 imagens ( set de treino e de validação). Em cada um dos sets, teremos 50 casos positivos e 50 casos negativos para garantir o balanceamento das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Quando temos o mesmo número de classes positivas e negativas num problema de classificação binário,nós usamos as seguintes métricas:\n",
    "> * Accuracy\n",
    "> * Precision\n",
    "> * Recall\n",
    "> * F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy ou Acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** ou acurácia é a métrica mais simples usada em machine learning. É definido como o quão preciso o modelo é em geral. Para o problema anterior, se a gente constroi um modelo que classifica **90 imagens corretamente**, nossa **accuracy (ou acurácia) é de 90% ou 0.9**. Se somente 83 imagens são classificadas corretamente, a accuracy é de 83% ou 0.83."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos calcular accuracy com Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    # Inicializa o contador para previsões corretas\n",
    "    correct = 0\n",
    "    \n",
    "    # loop sobre todos elementos de y_true e y_pred \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct += 1\n",
    "    \n",
    "    # retorna o avalor da accuracy que é a quantidade de previsões corretas sobre o total\n",
    "    return (correct/ len(y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.625\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,1,1,1,0,0,0,1]\n",
    "y_pred = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "accuracy_ = accuracy(y_true, y_pred)\n",
    "print(f\"Accuracy : {accuracy_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Povemos também realizar o cálculo da acurácia com o scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.625\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0,1,1,1,0,0,0,1]\n",
    "y_pred = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "accuracy_sklearn = metrics.accuracy_score(y_true,y_pred)\n",
    "\n",
    "print(f\"Accuracy : {accuracy_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Desbalanceado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos considerar que temos um dataset desbalanceado com 180 imagens de pacientes que não possuem pneumatórax e apenas 20 que possuem.Mesmo para esse caso, nós iremos criar os datasets de treinamento e validação com as mesmas proporções de targets positivos e negativos. Em cada set, teremos 90 imagems sem pneumatórax e 10 imagens com pneumatórax. *Para a situação em questão, se você disser que todas as imagens do set de validação são de pacientes que não possuem pneumatórax, a acurácia seria de 90%!*\n",
    "\n",
    "É notório que temos classes desbalanceadas com uma delas significativamente maior que a outra.**Nesse caso, não é recomendado usar a acurácia como métrica de avaliação pois ela não é representativa para os dados em questão**. Mesmo que tenhamos uma acurácia grande, o modelo provavelmente terá uma performance pobre quando for aplicado para dados de produção e você terá sérios problemas para explicar para o seu gerente o motivo.\n",
    "\n",
    "**Em casos como esse, é melhor olhar para outras métricas como por exemplo a precision.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de falar em precision, vamos entender outros termos importantes. Vamos assumir que imagem com pneumatórax são da classe positiva (1) e imagens sem são da classe negativa (0).\n",
    "\n",
    "**True Positive (TP) ou Verdadeiro Positivo (VP)**: Dada uma imagem, se o modelo prever que tal imagem possui pneumatórax e o valor de target da imagem mostra que o paciente realmente possui pneumatórax, é considerado true positive ou verdadeiro positivo.\n",
    "\n",
    "**True Negative (TN) ou Verdadeiro Negativo (VN)**: Dada uma imagem, se o modelo prever que essa imagem não possui pneumatórax e a imagem é de um paciente que realmente não possui pneumatórax, é considerado True Negative.\n",
    "\n",
    "Resumindo:\n",
    "* *Se o modelo prever corretamente classe positiva, é* **True Positive**\n",
    "* *Se o modelo prever corretamente classe negativa, é* **True Negative**\n",
    "\n",
    "**False Positive (FP) ou Falso Positivo**: Dada uma imagem, se o modelo prever pneumatórax porém a imagem é de um paciente sem pneumatórax, é um False Positive.\n",
    "\n",
    "**False Negative (FN) ou Falso Negativo**: Dada uma imagem, se o modelo prever que a imagem não é de paciente com pneumatórax, porém o paciente possui pneumatórax, é um False Negative.\n",
    "\n",
    "Resumindo:\n",
    "* *Se o modelo prever incorretamente classe positiva, é* **False Positive**\n",
    "* *Se o modelo prever incorretamente classe negativa, é* **False Negative**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos olhar a implementação em python dos conceitos apresentados acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    true_pos = 0\n",
    "    for yt,yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            true_pos += 1\n",
    "    \n",
    "    return true_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_negative(y_true,y_pred):\n",
    "    true_neg = 0\n",
    "    for yt,yp in zip(y_true,y_pred):\n",
    "        if yt == 0 and yp ==0:\n",
    "            true_neg += 1\n",
    "            \n",
    "    return true_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive(y_true, y_pred):\n",
    "    false_pos = 0\n",
    "    for yt,yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp ==1:\n",
    "            false_pos += 1\n",
    "            \n",
    "    return false_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negative(y_true, y_pred):\n",
    "    false_neg = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            false_neg += 1\n",
    "            \n",
    "    return false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota: As funções aplicadas acima funcionam apenas para classificação binária.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Y = [0,1,1,1,0,0,0,1]\n",
    "y_Pred = [0,1,0,1,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive(true_Y, y_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive(true_Y, y_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negative(true_Y, y_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_negative(true_Y, y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se formos definir **Accuracy** com os termos apresentados acima, nós teríamos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Accuracy Score = \\frac{ (TP + TN)}{(TP + TN+FP + FN)}\\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que conhecemos todos esses conceito, podemos calcular a acurácia usingo TP, TM, FP, e FN em python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_v2(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    \n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    return accuracy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agora vamos comparar os resultados com a versao do *scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Versão 1:  0.625\n",
      "Accuracy Versão 2:  0.625\n",
      "Accuracy Versão sklearn:  0.625\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,1,1,1,0,0,0,1]\n",
    "y_pred = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "accuracy_1 = accuracy(y_true, y_pred)\n",
    "print('Accuracy Versão 1: ', accuracy_1)\n",
    "\n",
    "accuracy_2 = accuracy_v2(y_true, y_pred)\n",
    "print('Accuracy Versão 2: ', accuracy_2)\n",
    "\n",
    "from sklearn import metrics\n",
    "accuracy_scikit_learn = metrics.accuracy_score(y_true, y_pred)\n",
    "print('Accuracy Versão sklearn: ', accuracy_scikit_learn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ótimos, agora vimos três maneiras de calcular acurácia e estão todas dando o mesmo resultado.\n",
    "\n",
    "Agora vamos conhecer melhor e avaliar outras métricas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira será **Precision**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Precision = \\frac{ TP }{(TP +FP)}\\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos supor que construímos um novo modelo em dados que também são desbalanceados. Diante dessas condições, nosso modelo identificou corretamente 80 casos de não pneumatórax num total de 90 e 8 casos de pneumatórax num total de 10. Consequentemente, o modelo identificou corretamente 88 imagens num total de 100. A **acurácia** nesse caso é de 0,88 ou 88%.\n",
    "\n",
    "Porém do total de 100 imagens, 10 images sem pneumatórax são classificadas incorretamente como casos com pneumatórax. Além disso, 2 casos de pneumatórax foram  classificados incorretamente como sem pneumatórax.\n",
    "\n",
    "Diante disso, temos:\n",
    ">  - TP: 8\n",
    ">  - TN: 80\n",
    ">  - FP: 10\n",
    ">  - FN: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa **Precision** será:\n",
    "    \n",
    "\\begin{equation*}\n",
    "Precision = \\frac{ 8 }{(8 +10)}= 0,444\\\\ \n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Isso significa que nosso modelo está correto 44% das vezes quando tenta identificar casos de pneumatórax.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos aplicar precision em python:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,1,1,1,0,0,0,1]\n",
    "y_pred = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "precision_ = precision(y_true, y_pred)\n",
    "print('precision:', precision_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos analisar **Recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** é definido como:\n",
    "    \n",
    "\\begin{equation*}\n",
    "Recall = \\frac{ TP }{(TP + FN)}\\\\ \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa **Recall** será:\n",
    "    \n",
    "\\begin{equation*}\n",
    "Precision = \\frac{ 8 }{(8 + 2)}= 0,80\\\\ \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Isso significa que nosso modelo identificou 80% dos casos positivos de pneumatórax corretamente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vamos aplicar o cálculo em python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    recall = tp/ (tp + fn)\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,1,1,1,0,0,0,1]\n",
    "y_pred = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "recall_ = recall(y_true, y_pred)\n",
    "print('recall:', recall_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para um bom modelo de machine learning, os valores de precision e recall precisam ser altos. Podemos observar isso no caso anterior, o valor de recall é 'um pouco' alto. Em contrapartida o valor de precision é muito baixo!\n",
    "\n",
    "Nosso modelo reproduz bastante falso positivos porém menos faso negativos. Poucos casos de falso negativos é bom nesse tipo de problema pois não queremos dizer que um paciente não possui pneumatórax quando na verdade ele possui. Isso pode proporcionar sérios problemas ! Porém ainda temos bastante falso positivos e isso também pode ser um problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grande parte dos modelos prevê probabilidade. Nessas predições nós geralmente escolhemos um threshold de 0,5. Esse limite (threshold) nao é sempre ideal e dependendo do valor escolhido para esse limite, os valores de precision e recall podem mudar drasticamente. Se para cada threshold que a gente escolher, calcularmos os valores de precision e recall, nos criamos uma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva de Precision e Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos assumir que temos duas listas:\n",
    "* y_true: os valores de targets\n",
    "* y_pred: Valores de probabilidade da amostra em questão ser da classe 1\n",
    "\n",
    "Agora vamos olhar as probabilidades de predições ao invés do valor que foi predito ( o que é calculado na grande maioria das vezes considerando um limite de 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,0,0,1,0,0,0,0,0,0,\n",
    "          1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "\n",
    "y_pred = [.02638412, 0.11114267, 0.31620708, \n",
    "            0.0490937, 0.0191491, 0.17554844, \n",
    "            0.15952202, 0.03819563, 0.11639273, \n",
    "            0.079377, 0.08584789, 0.39095342, \n",
    "            0.27259048, 0.03447096, 0.04644807, \n",
    "            0.03543574, 0.18521942, 0.05934905, \n",
    "            0.61977213, 0.33056815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(y_true))\n",
    "\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# limites ou thresholds\n",
    "thresholds = [0.0490937, 0.05934905, 0.079377,\n",
    "              0.08584789, 0.11114267, 0.11639273, \n",
    "              0.15952202, 0.17554844, 0.18521942, \n",
    "              0.27259048, 0.31620708, 0.33056815, \n",
    "              0.39095342, 0.61977213] \n",
    "\n",
    "# Para cada threshold, vamos calcular a predição em binário\n",
    "# e guardar os valores de precision e recall em duas listas distintas\n",
    "\n",
    "# Para cada valor dentro da lista y_pred, vamos verificar se esse valor é maior \n",
    "# ou igual a cada valor da lista de threshold. Se for, temp+prediction\n",
    "# adiciona 1 a lista, caso contrário adiciona 0\n",
    "for i in thresholds:\n",
    "    temp_prediction = [1 if x >= i else 0 for x in y_pred]\n",
    "\n",
    "    p = precision(y_true, temp_prediction)\n",
    "    r = recall( y_true, temp_prediction)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora plota os valores de precision e recall.\n",
    "\n",
    "### A curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(recalls, precisions)\n",
    "plt.xlabel('Recall', fontsize = 15)\n",
    "plt.ylabel('Precision', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cuva acima de precision e recall pode parecer diferente do que você está familiarizado a ver. Essa diferença ocorre devido ao fato de termos somente 20 dados com apensas 3 valores positivos. Porém não se preocupe, ela é a mesma curva de precision e recall que você está acostumado a ver pela internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A figura acima mostra que é difícil escolher um valor de threshold que nos de um bom valor de precision e recall.**\n",
    "\n",
    "Se o limite (threshold) é muito alto, teremos um pequeno número de dados **True Positive** e um **grande valor de falso negativo**. Isso diminui o valor de **Recall**, apensar disso o score de precision será mais alto.\n",
    "\n",
    "Por outro lado, se escolhermos um valor muito pequeno de threshold, o número de **false positives** irá crescer siginificativamente e a **precision** será menor.\n",
    "\n",
    "Esses valores de precision e recall variam entre 0 a 1 e quanto mais o score for próximo de 1, melhor !\n",
    "\n",
    "**F1 Score é a métrica que combina precision e recall.** É definida como uma média ponderada simples ou média harmônica da precision e do recall. Se considerarmos precision como P e recall como R, Podemos representar a **F1 Score** como:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score** \n",
    "    \n",
    "\\begin{equation*}\n",
    "F1 = \\frac{ 2PR }{(P+ R)}\\\\ \n",
    "\\end{equation*}\n",
    "\n",
    "Com um pouco de matemática chegamos a:\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = \\frac{ 2TP }{(2TP + FP + FN)}\\\\ \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em python temos:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    \n",
    "    f1_score = 2*p*r/ (p + r )\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0,0,0,1,0,0,0,0,0,0,\n",
    "         1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "\n",
    "y_pred = [0,0,1,0,0,0,1,0,0,0,\n",
    "         1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "f1(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparando com scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao invés de olhar para precision e recall individualmente, nós podemos apenas olhar para o F1 score. Assim como precision, recall e accuracy, o F1 score varia de 0 a 1 e um modelo perfeito teria F1 igual a 1. **Quando lidamos com datasets com classes desbalanceadas, nós devemos olhar para F1 (ou recision e recall) ao invés de olhar a acurácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate (TPR) e False Positive Rate (FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem outros termos importantes que precisamos saber que são **TPR ou True Positive Rate** e **FPR ou False Positive Rate**. Estes serão apresentados agora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate (TPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **TPR** \n",
    "    \n",
    "\\begin{equation*}\n",
    "TPR = \\frac{TP}{(TP+ FN)}\\\\ \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar da TPR ser a mesma coisa que Recall, vamos fazer uma função python para ela.\n",
    "\n",
    "* TPR ou Recall também é conhecido como **Sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr(y_true, y_pred):\n",
    "    return recall(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate (FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **FPR** \n",
    "    \n",
    "\\begin{equation*}\n",
    "FPR = \\frac{FP}{(TN + FP)}\\\\ \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(y_true,y_pred):\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    return fp/(tn + fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Negative Rate ou Specificity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **True Negative Rate ou Specificity** \n",
    "    \n",
    "\\begin{equation*}\n",
    "FPR = 1- \\frac{FP}{(TN + FP)}\\\\ \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São realmente muitos termos porém **os mais importantes são apenas TNR e FPR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva ROC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar que temos 15 dados e que os valores de targets são binários.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "y_true = [ 0,0,0,0,1,0,1,0,0,1,0,1,0,0,1 ]\n",
    "\n",
    "Vamos assumir que treinamos um modelo random forest, e temos a probabilidade de um determinado dado ser da classe positiva.\n",
    "\n",
    "prababilidade de ser 1 : [ 0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99 ] \n",
    "\n",
    "Para um valor de threshold  maior igual 0.5, nós podemos avaliar todos os valores de precision, recall/TPR, F1 score e FPR. Porém, Podemos fazer a mesma avaliação para diferentes valores de threshold. De fato, podemos escolher qualquer valor de 0 a 1 para calcular todas as métricas mensionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer essa avaliação levando em consideração somente duas métricas: TPR e FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "# Valor de target real\n",
    "y_true = [0,0,0,0,1,0,1,0,0,1,0,1,0,0,1]\n",
    "\n",
    "\n",
    "# Probabilidade do ser da classe positiva (1)\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99] \n",
    "\n",
    "# Threshold \n",
    "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0] \n",
    " \n",
    "for thresh in thresholds:\n",
    "    # Calcula a previsão para cada threshold\n",
    "    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n",
    "    # True Positive Rate\n",
    "    temp_tpr = tpr(y_true, temp_pred)\n",
    "    # False Positive Rate \n",
    "    temp_fpr = fpr(y_true, temp_pred) \n",
    "    # Inclui na lista \n",
    "    tpr_list.append(temp_tpr) \n",
    "    fpr_list.append(temp_fpr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
